{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3e6b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57daf23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff592e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame, functions as F\n",
    "from pyspark.sql.types import IntegerType,LongType,DoubleType,StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215441f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85922ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811325f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_file = '2022-11-04'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a082881",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111eca23",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/ubuntu/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e741b4f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f246e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "        .builder\n",
    "        .appName(\"OTUS\")\n",
    "        .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfae7eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459da7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/user/ubuntu/data/2022-11-04.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190a1797",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/user/ubuntu/data/2022-10-05.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9028541e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/user/ubuntu/data/*.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8027b3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.text(path)\n",
    "\n",
    "split_col = F.split(df['value'], ',')\n",
    "\n",
    "\n",
    "df_split = (df\n",
    "            .withColumn('tranaction_id', split_col.getItem(0))\n",
    "            .withColumn('tx_datetime', split_col.getItem(1))\n",
    "            .withColumn('customer_id', split_col.getItem(2))\n",
    "            .withColumn('terminal_id', split_col.getItem(3))\n",
    "            .withColumn('tx_amount', split_col.getItem(4))\n",
    "            .withColumn('tx_time_seconds', split_col.getItem(5))\n",
    "            .withColumn('tx_time_days', split_col.getItem(6))\n",
    "            .withColumn('tx_fraud', split_col.getItem(7))\n",
    "            .withColumn('tx_fraud_scenario', split_col.getItem(8))\n",
    "            .drop(*['value'])\n",
    "            .filter(~F.lower(F.col('tranaction_id')).like('%tranaction_id%'))\n",
    "            .withColumn('date_key', F.to_date(F.col('tx_datetime').cast(StringType())))\n",
    "            .withColumn('date_key', F.col('date_key').cast(StringType()))\n",
    "            \n",
    "#             .withColumn('hz', split_col.getItem(9))\n",
    "#             .withColumn('hz2', split_col.getItem(10))\n",
    "           )\n",
    "\n",
    "\n",
    "df_split.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c832e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a46d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7e9076",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_agg = (\n",
    "    df_split\n",
    "    .agg(\n",
    "        F.max(F.col('tx_datetime')).alias('max_time'),\n",
    "        F.min(F.col('tx_datetime')).alias('min_time'),\n",
    "        F.max(F.col('date_key')).alias('max_date'),\n",
    "        F.min(F.col('date_key')).alias('min_date'),\n",
    "        )\n",
    "    )\n",
    "\n",
    "sdf_agg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4238d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_agg = (\n",
    "    df_split\n",
    "    .agg(\n",
    "        F.max(F.col('tx_datetime')).alias('max_time'),\n",
    "        F.min(F.col('tx_datetime')).alias('min_time'),\n",
    "        F.max(F.col('date_key')).alias('max_date'),\n",
    "        F.min(F.col('date_key')).alias('min_date'),\n",
    "        )\n",
    "    )\n",
    "\n",
    "sdf_agg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c186160d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_path ='/user/ubuntu/data/'\n",
    "name_file = 'clean_data'\n",
    "output_hdfs_table_path = f'{hdfs_path}{name_file}.parquet'\n",
    "\n",
    "output_hdfs_table_path\n",
    "#mode= \"overwrite\"\n",
    "mode =\"append\"\n",
    "fmt= \"parquet\"\n",
    "partition_cols= (\"date_key\",),\n",
    "# output_hdfs_table_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bd3128",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = df_split.sample(0.1)\n",
    "sdf.printSchema()\n",
    "\n",
    "(sdf\n",
    " .write\n",
    " .format(fmt)\n",
    " .mode(mode)\n",
    " .partitionBy(*partition_cols)\n",
    " .save(output_hdfs_table_path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f0cf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(output_hdfs_table_path)\n",
    "df_from_parquet = spark.read.parquet(output_hdfs_table_path)\n",
    "\n",
    "df_from_parquet.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31807f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_agg = (\n",
    "    df_split\n",
    "    .agg(\n",
    "        F.max(F.col('tx_datetime')).alias('max_date'),\n",
    "        F.min(F.col('tx_datetime')).alias('min_date')\n",
    "        )\n",
    "    )\n",
    "\n",
    "sdf_agg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d87483",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split.sample(0.1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c96195",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf: DataFrame,\n",
    "    output_hdfs_table_path: str,\n",
    "    output_hdfs_table_name: Optional[str] = None,\n",
    "    partition_cols: Optional[Tuple[str]] = (\"part_date\",),\n",
    "    num_out_partitions: Optional[int] = 1,\n",
    "    fmt: str = \"parquet\",\n",
    "    mode: str = \"overwrite\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634dba96",
   "metadata": {},
   "outputs": [],
   "source": [
    "(sdf.write.format(fmt).mode(mode).partitionBy(*partition_cols).save(output_hdfs_table_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7cbcd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7870d5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_path ='/user/ubuntu/data/'\n",
    "name_file = '20221104'\n",
    "output_hdfs_table_path = f'{hdfs_path}{name_file}.parquet'\n",
    "\n",
    "output_hdfs_table_path\n",
    "mode= \"overwrite\"\n",
    "fmt= \"parquet\"\n",
    "partition_cols= (\"date_key\",),\n",
    "# output_hdfs_table_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb281c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = df_split.sample(0.1)\n",
    "\n",
    "(sdf\n",
    " .write\n",
    " .format(fmt)\n",
    " .mode(mode)\n",
    " .partitionBy(*partition_cols)\n",
    " .save(output_hdfs_table_path))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d415af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_agg = (\n",
    "    df_split\n",
    "    .select(*['tx_amount'])\n",
    "    .dropna()\n",
    "    .agg(\n",
    "        F.round(F.expr(\"percentile(tx_amount, array(0.01))\")[0], 4).alias(\"tx_amount_1perc\"),\n",
    "        F.round(F.expr(\"percentile(tx_amount, array(0.99))\")[0], 4).alias(\"tx_amount_99perc\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "sdf_agg.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4c82bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_agg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a0bc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = [\n",
    "    'tranaction_id',\n",
    "    'tx_datetime',\n",
    "    'customer_id',\n",
    "    'terminal_id',\n",
    "    'tx_amount',\n",
    "    'tx_time_seconds',\n",
    "    'tx_time_days',\n",
    "    'tx_fraud',\n",
    "    'tx_fraud_scenario'\n",
    "]\n",
    "\n",
    "sdf_clean = (\n",
    "        df_split\n",
    "        .join(sdf_agg, how='left')\n",
    "        .filter(F.col('tx_amount')>0)\n",
    "        .filter(F.col('tx_amount')>=F.col('tx_amount_1perc'))\n",
    "        .filter(F.col('tx_amount')<=F.col('tx_amount_99perc'))\n",
    "        .dropna()\n",
    "        .filter(~F.col('terminal_id').rlike('[a-zA-Z]')) # только альфа-нумерик\n",
    "        .filter(~F.col('customer_id').rlike('[a-zA-Z]')) # только альфа-нумерик\n",
    "        .filter(~F.col('tranaction_id').rlike('[a-zA-Z]')) # только альфа-нумерик\n",
    "        .filter(~F.col('tx_fraud').rlike('[a-zA-Z]')) # только альфа-нумерик\n",
    "        .filter(~F.col('tx_fraud_scenario').rlike('[a-zA-Z]')) # только альфа-нумерик\n",
    "        .filter(F.col('tx_time_seconds')>0) # только альфа-нумерик\n",
    "        .filter(F.col('tx_time_days')>0) # только альфа-нумерик\n",
    "        .select(*col_list)\n",
    "        .distinct()\n",
    "        .withColumn('tranaction_id', F.col('tranaction_id').cast(StringType()))\n",
    "        .withColumn('tx_datetime', F.col('tx_datetime').cast(StringType()))\n",
    "        .withColumn('customer_id', F.col('customer_id').cast(StringType()))\n",
    "        .withColumn('terminal_id', F.col('terminal_id').cast(StringType()))\n",
    "        .withColumn('tx_amount', F.col('tx_amount').cast(DoubleType()))\n",
    "        .withColumn('tx_time_seconds', F.col('tx_time_seconds').cast(LongType()))\n",
    "        .withColumn('tx_time_days', F.col('tx_time_days').cast(LongType()))\n",
    "        .withColumn('tx_fraud', F.col('tx_fraud').cast(IntegerType()))\n",
    "        .withColumn('tx_fraud_scenario', F.col('tx_fraud_scenario').cast(IntegerType()))\n",
    "        .withColumn('part_date', F.lit(name_file))\n",
    ")\n",
    "\n",
    "sdf_clean.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a28364",
   "metadata": {},
   "outputs": [],
   "source": [
    "#после удаления перцентилей\n",
    "sdf_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b44c694",
   "metadata": {},
   "outputs": [],
   "source": [
    "46998983"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8f0b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#после удаления пустых и текстовых и перцентилей\n",
    "sdf_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fa558f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#после удаления пустых и текстовых и перцентилей\n",
    "sdf_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1feac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_cols = 'part_date'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ad53d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "(\n",
    "    sdf_clean\n",
    "        .repartition(10)\n",
    "        .write\n",
    "        #.partitionBy(*partition_cols)\n",
    "        .mode(\"overwrite\")\n",
    "        .parquet(output_hdfs_table_path)\n",
    "        #.parquet(\"data/train.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d01568",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls -h data/train.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76d4b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_path = path_hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942a794c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/ubuntu/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04192cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_path ='/user/ubuntu/data/'\n",
    "name_file = '20221104'\n",
    "output_hdfs_table_path = f'{hdfs_path}{name_file}.parquet'\n",
    "output_hdfs_table_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88ff497",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/ubuntu/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74130193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cd026f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/ubuntu/data/20221104.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccb6869",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a464423",
   "metadata": {},
   "outputs": [],
   "source": [
    "#проверка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f374e81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_parquet = spark.read.parquet(output_hdfs_table_path)\n",
    "\n",
    "df_from_parquet.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7641a50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_parquet.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0b51d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)  # to pretty print pyspark.DataFrame in jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c0ae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_parquet.limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25b021e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_parquet.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d96c735",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57cf0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls -h /user/ubuntu/data/20221104.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427e4e2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5e9d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_parquet.groupBy(['tx_fraud']).agg(F.sum(F.lit(1))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4c7f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_parquet.groupBy(['tx_fraud_scenario']).agg(F.sum(F.lit(1))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a046b07d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c2866a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'otus-bucket-b1gcs058gd4fa8eeviec'\n",
    "input_path = f\"s3a://{bucket_name}/*.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48a9bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'otus-bucket-b1gcs058gd4fa8eeviec'\n",
    "input_path = f\"s3a://{bucket_name}/*.txt\"\n",
    "\n",
    "df = spark.read.text(input_path)\n",
    "\n",
    "split_col = F.split(df['value'], ',')\n",
    "\n",
    "\n",
    "df_split = (df\n",
    "            .withColumn('tranaction_id', split_col.getItem(0))\n",
    "            .withColumn('tx_datetime', split_col.getItem(1))\n",
    "            .withColumn('customer_id', split_col.getItem(2))\n",
    "            .withColumn('terminal_id', split_col.getItem(3))\n",
    "            .withColumn('tx_amount', split_col.getItem(4))\n",
    "            .withColumn('tx_time_seconds', split_col.getItem(5))\n",
    "            .withColumn('tx_time_days', split_col.getItem(6))\n",
    "            .withColumn('tx_fraud', split_col.getItem(7))\n",
    "            .withColumn('tx_fraud_scenario', split_col.getItem(8))\n",
    "            .drop(*['value'])\n",
    "            .filter(~F.lower(F.col('tranaction_id')).like('%tranaction_id%'))\n",
    "            .withColumn('date_key', F.to_date(F.col('tx_datetime')))\n",
    "            .withColumn('date_key', F.col('date_key').cast(StringType()))\n",
    "            \n",
    "            \n",
    "#             .withColumn('hz', split_col.getItem(9))\n",
    "#             .withColumn('hz2', split_col.getItem(10))\n",
    "           )\n",
    "\n",
    "\n",
    "df_split.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15456354",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.text(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c51266",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW PARTITIONS customer;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f0d302",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_hdfs_table_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c9b277",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = '/user/ubuntu/data/20221104.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f0adc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_from_parquet = spark.read.parquet(result_path)\n",
    "\n",
    "\n",
    "print(output_hdfs_table_path)\n",
    "df_from_parquet = spark.read.parquet(output_hdfs_table_path)\n",
    "\n",
    "df_from_parquet.printSchema()\n",
    "\n",
    "df_from_parquet2 = (df_from_parquet\n",
    "                    .select('date_key')\n",
    "                    .distinct()\n",
    "                    .withColumn('date_key', F.col('date_key').cast(StringType()))\n",
    "                   )\n",
    "\n",
    "df_from_parquet2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b8e165",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f622e950",
   "metadata": {},
   "outputs": [],
   "source": [
    "exist_partition = df_from_parquet2.select('date_key').distinct().collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbec365",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_partitions = [str(row[0]) for row in exist_partition]\n",
    "    \n",
    "ex_partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0210d5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузим новый файл и проверим, что что-то есть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5378a473",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'otus-bucket-b1gcs058gd4fa8eeviec'\n",
    "input_path = f\"s3a://{bucket_name}/*.txt\"\n",
    "\n",
    "df = spark.read.text(input_path)\n",
    "\n",
    "split_col = F.split(df['value'], ',')\n",
    "\n",
    "\n",
    "df_split = (df\n",
    "            .withColumn('tranaction_id', split_col.getItem(0))\n",
    "            .withColumn('tx_datetime', split_col.getItem(1))\n",
    "            .withColumn('customer_id', split_col.getItem(2))\n",
    "            .withColumn('terminal_id', split_col.getItem(3))\n",
    "            .withColumn('tx_amount', split_col.getItem(4))\n",
    "            .withColumn('tx_time_seconds', split_col.getItem(5))\n",
    "            .withColumn('tx_time_days', split_col.getItem(6))\n",
    "            .withColumn('tx_fraud', split_col.getItem(7))\n",
    "            .withColumn('tx_fraud_scenario', split_col.getItem(8))\n",
    "            .drop(*['value'])\n",
    "            .filter(~F.lower(F.col('tranaction_id')).like('%tranaction_id%'))\n",
    "            .withColumn('date_key', F.to_date(F.col('tx_datetime')))\n",
    "            .withColumn('date_key', F.col('date_key').cast(StringType()))\n",
    "            \n",
    "            \n",
    "#             .withColumn('hz', split_col.getItem(9))\n",
    "#             .withColumn('hz2', split_col.getItem(10))\n",
    "           )\n",
    "\n",
    "\n",
    "df_split.show(3)\n",
    "\n",
    "df_split.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec0e8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27646544",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split.filter(~F.col('date_key').isin(ex_partitions)).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e660d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34447bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = df_split.filter(~F.col('date_key').isin(ex_partitions)).sample(0.1)\n",
    "sdf.printSchema()\n",
    "\n",
    "(sdf\n",
    " .write\n",
    " .format(fmt)\n",
    " .mode(mode)\n",
    " .partitionBy(*partition_cols)\n",
    " .save(output_hdfs_table_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf3a76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_hdfs_table_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d125d8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(output_hdfs_table_path)\n",
    "df_from_parquet = spark.read.parquet(output_hdfs_table_path)\n",
    "\n",
    "df_from_parquet.printSchema()\n",
    "\n",
    "df_from_parquet2 = (df_from_parquet\n",
    "                    .select('date_key')\n",
    "                    .distinct()\n",
    "                    .withColumn('date_key', F.col('date_key').cast(StringType()))\n",
    "                   )\n",
    "\n",
    "df_from_parquet2.printSchema()\n",
    "\n",
    "exist_partition = df_from_parquet2.select('date_key').distinct().collect()\n",
    "\n",
    "ex_partitions_new = [str(row[0]) for row in exist_partition]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21b1522",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_partitions_new "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55c1da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_partitions_new "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2746da8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_parquet.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627d9312",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(93996170+46993904)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ba69c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6549e003",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3a://{bucket_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830018a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls -h /user/ubuntu/data/20221104.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b59942f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3cmd --config=/home/ubuntu/.s3cfg ls s3://cold-s3-bucket/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf282302",
   "metadata": {},
   "outputs": [],
   "source": [
    "!s3cmd --config=/home/ubuntu/.s3cfg ls s3://cold-s3-bucket/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57beafc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9631c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'otus-bucket-b1gcs058gd4fa8eeviec'\n",
    "input_path = f\"s3a://{bucket_name}/*.txt\"\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "\n",
    "\n",
    "    df = spark.read.text(input_path)\n",
    "\n",
    "    split_col = F.split(df['value'], ',')\n",
    "\n",
    "\n",
    "    partitions = (df\n",
    "#                 .withColumn('tranaction_id', split_col.getItem(0))\n",
    "                .withColumn('tx_datetime', split_col.getItem(1))\n",
    "#                 .withColumn('customer_id', split_col.getItem(2))\n",
    "#                 .withColumn('terminal_id', split_col.getItem(3))\n",
    "#                 .withColumn('tx_amount', split_col.getItem(4))\n",
    "#                 .withColumn('tx_time_seconds', split_col.getItem(5))\n",
    "#                 .withColumn('tx_time_days', split_col.getItem(6))\n",
    "#                 .withColumn('tx_fraud', split_col.getItem(7))\n",
    "#                 .withColumn('tx_fraud_scenario', split_col.getItem(8))\n",
    "#                 .drop(*['value'])\n",
    "#                 .filter(~F.lower(F.col('tranaction_id')).like('%tranaction_id%'))\n",
    "                .withColumn('date_key', F.to_date(F.col('tx_datetime')))\n",
    "                .withColumn('date_key', F.col('date_key').cast(StringType()))\n",
    "                .select('date_key')\n",
    "                .distinct()\n",
    "               ).collect()\n",
    "\n",
    "    ex_partitions = [str(row[0]) for row in partitions]\n",
    "except Exception:\n",
    "    ex_partitions = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a094cee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a2c086",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_parquet = spark.read.parquet(output_hdfs_table_path)\n",
    "\n",
    "partitions = (df_from_parquet\n",
    "                    .select('date_key')\n",
    "                    .distinct()\n",
    "                    .withColumn('date_key', F.col('date_key').cast(StringType()))\n",
    "                    .distinct()\n",
    "                   ).collect()\n",
    "\n",
    "ex_partitions = [str(row[0]) for row in partitions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d18e86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb382f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfbd5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11209550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae4ff8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 cleaning_data2.py --bucket 'otus-bucket-b1gcs058gd4fa8eeviec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abe3dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_parquet2 = (df_from_parquet\n",
    "                    .select('date_key')\n",
    "                    .distinct()\n",
    "                    .withColumn('date_key', F.col('date_key').cast(StringType()))\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763c0c23",
   "metadata": {},
   "outputs": [],
   "source": [
    " s3a://otus-bucket-b1gcs058gd4fa8eeviec/input/*.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6212d281",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'otus-bucket-b1gcs058gd4fa8eeviec'\n",
    "input_path = f\"s3a://{bucket_name}/*.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484e72fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c925dc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/user/ubuntu/data/2022-11-04.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bb892c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_parquet = spark.read.parquet(output_hdfs_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74f46e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349205ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'cold-s3-bucket'\n",
    "input_path = f\"s3a://{bucket_name}/output_data/clean_data.parquet\"\n",
    "\n",
    "\n",
    "sdf = spark.read.parquet(input_path)\n",
    "\n",
    "sdf.printSchema()\n",
    "# try:\n",
    "\n",
    "\n",
    "#     df = spark.read.text(input_path)\n",
    "\n",
    "#     split_col = F.split(df['value'], ',')\n",
    "\n",
    "\n",
    "#     partitions = (df\n",
    "# #                 .withColumn('tranaction_id', split_col.getItem(0))\n",
    "#                 .withColumn('tx_datetime', split_col.getItem(1))\n",
    "# #                 .withColumn('customer_id', split_col.getItem(2))\n",
    "# #                 .withColumn('terminal_id', split_col.getItem(3))\n",
    "# #                 .withColumn('tx_amount', split_col.getItem(4))\n",
    "# #                 .withColumn('tx_time_seconds', split_col.getItem(5))\n",
    "# #                 .withColumn('tx_time_days', split_col.getItem(6))\n",
    "# #                 .withColumn('tx_fraud', split_col.getItem(7))\n",
    "# #                 .withColumn('tx_fraud_scenario', split_col.getItem(8))\n",
    "# #                 .drop(*['value'])\n",
    "# #                 .filter(~F.lower(F.col('tranaction_id')).like('%tranaction_id%'))\n",
    "#                 .withColumn('date_key', F.to_date(F.col('tx_datetime')))\n",
    "#                 .withColumn('date_key', F.col('date_key').cast(StringType()))\n",
    "#                 .select('date_key')\n",
    "#                 .distinct()\n",
    "#                ).collect()\n",
    "\n",
    "#     ex_partitions = [str(row[0]) for row in partitions]\n",
    "# except Exception:\n",
    "#     ex_partitions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4114e959",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43be4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bab58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "138174127*0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dee4cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d57821",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sdf.sample(0.002).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4e07b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c864f7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('dt_test_sample.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8734b3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tx_fraud.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78231c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee9e76b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64000ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'cold-s3-bucket'\n",
    "input_path = f\"s3a://{bucket_name}/output_data/clean_data.parquet\"\n",
    "\n",
    "\n",
    "sdf = spark.read.parquet(input_path)\n",
    "\n",
    "sdf.printSchema()\n",
    "# try:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc4aece",
   "metadata": {},
   "source": [
    "# agg 1 customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0338cb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dates= sdf.select('date_key').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939f414b",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d042443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "str(list_dates[0][0])\n",
    "\n",
    "ld = [str(x[0]) for x in list_dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4141edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=min(ld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6954ec40",
   "metadata": {},
   "outputs": [],
   "source": [
    "end=max(ld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86829657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e93ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_keys = [\n",
    "    time_key.strftime(\"%Y-%m-%d\") for time_key\n",
    "    in pd.date_range(start, end, freq='1D')\n",
    "]\n",
    "time_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e81a642",
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_date in time_keys[20:21]:\n",
    "    print(target_date)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7a03ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9116fae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep = 4\n",
    "sdf_cut_per = (sdf\n",
    "                    .withColumn('target_date', F.lit(target_date))\n",
    "                    .withColumn('target_date', F.to_date(F.col('target_date'), 'yyyy-MM-dd'))\n",
    "                    .filter(F.col('date_key') >= F.date_sub(F.col('target_date'),deep))\n",
    "                    .filter(F.col('date_key') <= F.date_sub(F.col('target_date'),1))             \n",
    "                    )\n",
    "sdf_cut_per.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb42eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-6\n",
    "deep = 4\n",
    "bucket_name = 'cold-s3-bucket'\n",
    "input_path = f\"s3a://{bucket_name}/output_data/clean_data.parquet\"\n",
    "\n",
    "\n",
    "sdf = spark.read.parquet(input_path)\n",
    "\n",
    "#for target_date in tqdm(time_keys[20:24]):\n",
    "for target_date in tqdm(time_keys[24:26]):\n",
    "    #print(target_date)\n",
    "\n",
    "    sdf_cut_per = (sdf\n",
    "                    .withColumn('target_date', F.lit(target_date))\n",
    "                    .withColumn('target_date', F.to_date(F.col('target_date'), 'yyyy-MM-dd'))\n",
    "                    .filter(F.col('date_key') >= F.date_sub(F.col('target_date'),deep))\n",
    "                    .filter(F.col('date_key') <= F.date_sub(F.col('target_date'),1))             \n",
    "                    )\n",
    "\n",
    "\n",
    "    #sdf_cut_per.printSchema()\n",
    "\n",
    "    sdf_customer = (sdf_cut_per\n",
    "                    .groupBy(['customer_id','target_date'])\n",
    "                    .agg(\n",
    "                        F.countDistinct(F.col('date_key')).alias('cust_active_days_7d'),\n",
    "                        F.countDistinct(F.col('terminal_id')).alias('cust_uniq_terminal_7d'),\n",
    "                        #F.round(F.expr('percentile(tx_amount, 0.05)'), 0).alias('cust_amount_5perc'),\n",
    "                        #F.round(F.expr('percentile(tx_amount, 0.25)'), 0).alias('cust_amount_25perc'),\n",
    "                        F.round(F.expr('percentile(tx_amount, 0.5)'), 0).alias('cust_amount_50perc'),\n",
    "                        #F.round(F.expr('percentile(tx_amount, 0.75)'), 0).alias('cust_amount_75perc'),\n",
    "                        #F.round(F.expr('percentile(tx_amount, 0.95)'), 0).alias('cust_amount_95perc'),\n",
    "                        F.max(F.col('tx_amount')).alias('cust_amount_max'),\n",
    "                        F.min(F.col('tx_amount')).alias('cust_amount_min'),\n",
    "                        F.sum(F.lit(1)).alias('cust_total_cnt_trans_7d'),                             \n",
    "                        F.sum(\n",
    "                            F.when(\n",
    "                                (F.col('tx_fraud') == 1) \n",
    "                                , F.lit(1)).otherwise(F.lit(0)))\n",
    "                                .alias('cust_bad_cnt_trans_7d'),\n",
    "                        )\n",
    "\n",
    "\n",
    "                    )\n",
    "\n",
    "    #sdf_customer.printSchema()\n",
    "\n",
    "\n",
    "    sdf_customer_per_day = (sdf_cut_per\n",
    "                    .groupBy(['customer_id','target_date','date_key'])\n",
    "                    .agg(\n",
    "                        F.sum(F.col('tx_amount')).alias('sum_amount_in_day'),\n",
    "                        F.sum(F.lit(1)).alias('cnt_in_day'),\n",
    "                        F.sum(\n",
    "                            F.when(\n",
    "                                (F.col('tx_fraud') == 1) \n",
    "                                , F.lit(1)).otherwise(F.lit(0)))\n",
    "                                .alias('bad_days'),\n",
    "\n",
    "                        )\n",
    "                    .withColumn('bad_days_ind', \n",
    "                        F.when(F.col('bad_days') > 0, F.lit(1))\n",
    "                        .otherwise(F.lit(0)))\n",
    "                    .groupBy(['customer_id','target_date'])\n",
    "                    .agg(  \n",
    "                        F.avg(F.col('sum_amount_in_day')).alias('cust_avg_amount_in_day_7d'),        \n",
    "                        F.avg(F.col('cnt_in_day')).alias('cust_cnt_in_day_7d'), \n",
    "                        F.sum(F.col('bad_days_ind')).alias('cust_days_with_bad_trans_7d'),\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "    #sdf_customer_per_day.printSchema()\n",
    "\n",
    "\n",
    "    sdf_customer_datamart = (sdf_customer\n",
    "                            .join(sdf_customer_per_day, on=['customer_id','target_date'], how='inner') \n",
    "                            .withColumnRenamed('target_date', 'date_key') \n",
    "#                             .withColumn(\"rel_cust_5perc\", (sdf_customer['cust_amount_5perc']-sdf_customer['cust_amount_min'])\\\n",
    "#                                / (sdf_customer['cust_amount_max'] - sdf_customer['cust_amount_min'] + eps))  \n",
    "#                             .withColumn(\"rel_cust_25perc\", (sdf_customer['cust_amount_25perc']-sdf_customer['cust_amount_min'])\\\n",
    "#                                / (sdf_customer['cust_amount_max'] - sdf_customer['cust_amount_min'] + eps))  \n",
    "                            .withColumn(\"rel_cust_50perc\", (sdf_customer['cust_amount_50perc']-sdf_customer['cust_amount_min'])\\\n",
    "                               / (sdf_customer['cust_amount_max'] - sdf_customer['cust_amount_min'] + eps)) \n",
    "#                             .withColumn(\"rel_cust_75perc\", (sdf_customer['cust_amount_75perc']-sdf_customer['cust_amount_min'])\\\n",
    "#                                / (sdf_customer['cust_amount_max'] - sdf_customer['cust_amount_min'] + eps)) \n",
    "#                             .withColumn(\"rel_cust_95perc\", (sdf_customer['cust_amount_95perc']-sdf_customer['cust_amount_min'])\\\n",
    "#                                / (sdf_customer['cust_amount_max'] - sdf_customer['cust_amount_min'] + eps)) \n",
    "                            )\n",
    "\n",
    "    num_out_partitions=1\n",
    "    \n",
    "    if num_out_partitions:\n",
    "        sdf_customer_datamart = sdf_customer_datamart.repartition(num_out_partitions)\n",
    "    \n",
    "\n",
    "    #sdf_customer_datamart.printSchema()\n",
    "    \n",
    "    bucket_name = 'cold-s3-bucket'\n",
    "    output_table_path = f\"s3a://{bucket_name}/output_data/agg_customer_data.parquet\"\n",
    "    \n",
    "#     hdfs_path ='/user/ubuntu/data/'\n",
    "#     name_file = 'agg_customer_data'\n",
    "#     output_table_path = f'{hdfs_path}{name_file}.parquet'\n",
    "\n",
    "    mode =\"append\"\n",
    "    fmt= \"parquet\"\n",
    "    partition_cols= (\"date_key\",),\n",
    "\n",
    "    (sdf_customer_datamart\n",
    "     .write\n",
    "     .format(fmt)\n",
    "     .mode(mode)\n",
    "     .partitionBy(*partition_cols)\n",
    "     .save(output_table_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bf6ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88610ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_keys[20:24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09229083",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_keys[24:26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcacf5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-6\n",
    "deep = 4\n",
    "bucket_name = 'cold-s3-bucket'\n",
    "input_path = f\"s3a://{bucket_name}/output_data/clean_data.parquet\"\n",
    "\n",
    "\n",
    "sdf = spark.read.parquet(input_path)\n",
    "\n",
    "# for target_date in tqdm(time_keys[20:24]):\n",
    "for target_date in tqdm(time_keys[24:26]):\n",
    "\n",
    "\n",
    "    sdf_cut_per = (sdf\n",
    "                    .withColumn('target_date', F.lit(target_date))\n",
    "                    .withColumn('target_date', F.to_date(F.col('target_date'), 'yyyy-MM-dd'))\n",
    "                    .filter(F.col('date_key') >= F.date_sub(F.col('target_date'),deep))\n",
    "                    .filter(F.col('date_key') <= F.date_sub(F.col('target_date'),1))             \n",
    "                    )\n",
    "\n",
    "\n",
    "    #sdf_cut_per.printSchema()\n",
    "\n",
    "    sdf_terminal = (sdf_cut_per\n",
    "                    .groupBy(['terminal_id','target_date'])\n",
    "                    .agg(\n",
    "                        F.countDistinct(F.col('date_key')).alias('term_active_days_7d'),\n",
    "                        F.countDistinct(F.col('customer_id')).alias('term_uniq_customer_7d'),\n",
    "                        #F.round(F.expr('percentile(tx_amount, 0.05)'), 0).alias('term_amount_5perc'),\n",
    "                        #F.round(F.expr('percentile(tx_amount, 0.25)'), 0).alias('term_amount_25perc'),\n",
    "                        F.round(F.expr('percentile(tx_amount, 0.5)'), 0).alias('term_amount_50perc'),\n",
    "                        #F.round(F.expr('percentile(tx_amount, 0.75)'), 0).alias('term_amount_75perc'),\n",
    "                        #F.round(F.expr('percentile(tx_amount, 0.95)'), 0).alias('term_amount_95perc'),\n",
    "                        F.sum(F.lit(1)).alias('term_total_cnt_trans_7d'),       \n",
    "                        F.max(F.col('tx_amount')).alias('term_amount_max'),\n",
    "                        F.min(F.col('tx_amount')).alias('term_amount_min'),\n",
    "                        F.sum(\n",
    "                            F.when(\n",
    "                                (F.col('tx_fraud') == 1) \n",
    "                                , F.lit(1)).otherwise(F.lit(0)))\n",
    "                                .alias('term_bad_cnt_trans_7d'),\n",
    "                        )\n",
    "\n",
    "                    )\n",
    "\n",
    "    #sdf_terminal.printSchema()\n",
    "\n",
    "\n",
    "    sdf_term_per_day = (sdf_cut_per\n",
    "                    .groupBy(['terminal_id','target_date','date_key'])\n",
    "                    .agg(\n",
    "                        F.sum(F.col('tx_amount')).alias('sum_amount_in_day'),\n",
    "                        F.sum(F.lit(1)).alias('cnt_in_day'),\n",
    "                        F.sum(\n",
    "                            F.when(\n",
    "                                (F.col('tx_fraud') == 1) \n",
    "                                , F.lit(1)).otherwise(F.lit(0)))\n",
    "                                .alias('bad_days'),\n",
    "\n",
    "                        )\n",
    "                    .withColumn('bad_days_ind', \n",
    "                        F.when(F.col('bad_days') > 0, F.lit(1))\n",
    "                        .otherwise(F.lit(0)))\n",
    "                    .groupBy(['terminal_id','target_date'])\n",
    "                    .agg(  \n",
    "                        F.avg(F.col('sum_amount_in_day')).alias('term_avg_amount_in_day_7d'),        \n",
    "                        F.avg(F.col('cnt_in_day')).alias('term_cnt_in_day_7d'), \n",
    "                        F.sum(F.col('bad_days_ind')).alias('term_days_with_bad_trans_7d'),\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "    #sdf_term_per_day.printSchema()\n",
    "\n",
    "\n",
    "    sdf_terminal_datamart = (sdf_terminal\n",
    "                            .join(sdf_term_per_day, on=['terminal_id','target_date'], how='inner') \n",
    "                            .withColumnRenamed('target_date', 'date_key') \n",
    "                            )\n",
    "\n",
    "\n",
    "    #sdf_terminal_datamart.printSchema()\n",
    "    \n",
    "    num_out_partitions=1\n",
    "    \n",
    "    if num_out_partitions:\n",
    "        sdf_terminal_datamart = sdf_terminal_datamart.repartition(num_out_partitions)\n",
    "    \n",
    "    bucket_name = 'cold-s3-bucket'\n",
    "    output_table_path = f\"s3a://{bucket_name}/output_data/agg_term_data.parquet\"\n",
    "    \n",
    "#     hdfs_path ='/user/ubuntu/data/'\n",
    "#     name_file = 'agg_customer_data'\n",
    "#     output_table_path = f'{hdfs_path}{name_file}.parquet'\n",
    "\n",
    "    mode =\"append\"\n",
    "    fmt= \"parquet\"\n",
    "    partition_cols= (\"date_key\",),\n",
    "\n",
    "    (sdf_terminal_datamart\n",
    "     .write\n",
    "     .format(fmt)\n",
    "     .mode(mode)\n",
    "     .partitionBy(*partition_cols)\n",
    "     .save(output_table_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a801f09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5997e8dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a8d7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239c3fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'cold-s3-bucket'\n",
    "output_table_path = f\"s3a://{bucket_name}/output_data/agg_customer_data.parquet\"\n",
    "\n",
    "mode =\"append\"\n",
    "fmt= \"parquet\"\n",
    "partition_cols= (\"date_key\",),\n",
    "\n",
    "(sdf\n",
    " .write\n",
    " .format(fmt)\n",
    " .mode(mode)\n",
    " .partitionBy(*partition_cols)\n",
    " .save(output_table_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c40731",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e96fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_path ='/user/ubuntu/data/'\n",
    "name_file = 'agg_customer_data'\n",
    "output_hdfs_table_path = f'{hdfs_path}{name_file}.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4524c98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e850c81d",
   "metadata": {},
   "source": [
    "## сборка обучающей выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f207fed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8d6be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_keys[20:26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534228f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6e704c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'cold-s3-bucket'\n",
    "row_path = f\"s3a://{bucket_name}/output_data/clean_data.parquet\"\n",
    "\n",
    "row_sdf = spark.read.parquet(row_path)\n",
    "\n",
    "row_sdf.printSchema()\n",
    "\n",
    "\n",
    "\n",
    "agg_cust_path = f\"s3a://{bucket_name}/output_data/agg_customer_data.parquet\"\n",
    "\n",
    "agg_cust_sdf = spark.read.parquet(agg_cust_path)\n",
    "\n",
    "agg_cust_sdf.printSchema()\n",
    "\n",
    "\n",
    "agg_term_path = f\"s3a://{bucket_name}/output_data/agg_term_data.parquet\"\n",
    "\n",
    "agg_term_sdf = spark.read.parquet(agg_term_path)\n",
    "\n",
    "agg_term_sdf.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13c4891",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_date  = time_keys[23]\n",
    "print(target_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ebec9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dbacc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469db207",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_for_fillna = [\n",
    "    'term_active_days_7d',\n",
    " 'term_uniq_customer_7d',\n",
    " 'term_amount_50perc',\n",
    " 'term_total_cnt_trans_7d',\n",
    " 'term_amount_max',\n",
    " 'term_amount_min',\n",
    " 'term_bad_cnt_trans_7d',\n",
    " 'term_avg_amount_in_day_7d',\n",
    " 'term_cnt_in_day_7d',\n",
    " 'term_days_with_bad_trans_7d',\n",
    " 'cust_active_days_7d',\n",
    " 'cust_uniq_terminal_7d',\n",
    " 'cust_amount_50perc',\n",
    " 'cust_amount_max',\n",
    " 'cust_amount_min',\n",
    " 'cust_total_cnt_trans_7d',\n",
    " 'cust_bad_cnt_trans_7d',\n",
    " 'cust_avg_amount_in_day_7d',\n",
    " 'cust_cnt_in_day_7d',\n",
    " 'cust_days_with_bad_trans_7d',\n",
    " 'rel_cust_50perc',\n",
    "#  'sh_bad_trans_per_cust',\n",
    "#  'sh_bad_trans_per_term',\n",
    "#  'sh_bad_days_per_cust',\n",
    "#  'sh_bad_days_per_term',\n",
    "#  'rel_cust_amount_to_max',\n",
    "#  'rel_term_amount_to_max'\n",
    "    \n",
    "    \n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94cd780",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-6\n",
    "\n",
    "target_date  = time_keys[23]\n",
    "\n",
    "def datamart(date_loc):\n",
    "    \n",
    "    eps = 1e-6\n",
    "\n",
    "    df = (row_sdf\n",
    "               .filter(F.col('date_key') == date_loc)\n",
    "               .join(agg_term_sdf, on=['terminal_id','date_key'], how='left')\n",
    "               .join(agg_cust_sdf, on=['customer_id','date_key'], how='left')\n",
    "               .fillna(value=0,subset=list_for_fillna)\n",
    "               #.withColumn(\"sh_bad_trans_per_cust\", sdf['cust_bad_cnt_trans_7d'] / (sdf.cust_total_cnt_trans_7d + eps))\n",
    "    #            .withColumn(\"sh_bad_trans_per_term\", sdf.term_bad_cnt_trans_7d / (sdf.term_total_cnt_trans_7d + eps))\n",
    "              )\n",
    "\n",
    "    #sdf.printSchema()\n",
    "\n",
    "    df = (df\n",
    "\n",
    "\n",
    "               .withColumn(\"sh_bad_trans_per_cust\", df['cust_bad_cnt_trans_7d'] / (df['cust_total_cnt_trans_7d'] + eps))\n",
    "               .withColumn(\"sh_bad_trans_per_term\", df['term_bad_cnt_trans_7d'] / (df['term_total_cnt_trans_7d'] + eps))\n",
    "               .withColumn(\"sh_bad_days_per_cust\", df['cust_days_with_bad_trans_7d'] / (df['cust_active_days_7d'] + eps))\n",
    "               .withColumn(\"sh_bad_days_per_term\", df['term_days_with_bad_trans_7d'] / (df['term_active_days_7d'] + eps))\n",
    "               .withColumn(\"sh_bad_trans_per_cust\", df['cust_bad_cnt_trans_7d'] / (df['cust_total_cnt_trans_7d'] + eps))\n",
    "               .withColumn(\"rel_cust_amount_to_max\", (df['tx_amount']-df['cust_amount_min'])\\\n",
    "                           / (df['cust_amount_max'] - df['cust_amount_min'] + eps)) \n",
    "               .withColumn(\"rel_term_amount_to_max\", (df['tx_amount']-df['term_amount_min'])\\\n",
    "                           / (df['term_amount_max'] - df['term_amount_min'] + eps))\n",
    "               .withColumnRenamed('tx_fraud', 'target') \n",
    "\n",
    "               )\n",
    "\n",
    "\n",
    "    #train_sdf.printSchema()\n",
    "    return df\n",
    "\n",
    "# sdf_mod.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af5f0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_keys[24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ab1eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sdf = datamart(time_keys[23])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ee06ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sdf = datamart(time_keys[24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c18defa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f4b5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ea3670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15fc11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dbe5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50ef237",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbc5eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-6\n",
    "\n",
    "target_date  = time_keys[24]\n",
    "\n",
    "sdf_mod = (row_sdf\n",
    "           .filter(F.col('date_key') == target_date)\n",
    "           .join(agg_term_sdf, on=['terminal_id','date_key'], how='left')\n",
    "           .join(agg_cust_sdf, on=['customer_id','date_key'], how='left')\n",
    "           .fillna(value=0,subset=list_for_fillna)\n",
    "           #.withColumn(\"sh_bad_trans_per_cust\", sdf['cust_bad_cnt_trans_7d'] / (sdf.cust_total_cnt_trans_7d + eps))\n",
    "#            .withColumn(\"sh_bad_trans_per_term\", sdf.term_bad_cnt_trans_7d / (sdf.term_total_cnt_trans_7d + eps))\n",
    "          )\n",
    "\n",
    "sdf.printSchema()\n",
    "\n",
    "train_sdf = (sdf_mod\n",
    "            \n",
    "            \n",
    "           .withColumn(\"sh_bad_trans_per_cust\", sdf_mod['cust_bad_cnt_trans_7d'] / (sdf_mod['cust_total_cnt_trans_7d'] + eps))\n",
    "           .withColumn(\"sh_bad_trans_per_term\", sdf_mod['term_bad_cnt_trans_7d'] / (sdf_mod['term_total_cnt_trans_7d'] + eps))\n",
    "           .withColumn(\"sh_bad_days_per_cust\", sdf_mod['cust_days_with_bad_trans_7d'] / (sdf_mod['cust_active_days_7d'] + eps))\n",
    "           .withColumn(\"sh_bad_days_per_term\", sdf_mod['term_days_with_bad_trans_7d'] / (sdf_mod['term_active_days_7d'] + eps))\n",
    "           .withColumn(\"sh_bad_trans_per_cust\", sdf_mod['cust_bad_cnt_trans_7d'] / (sdf_mod['cust_total_cnt_trans_7d'] + eps))\n",
    "           .withColumn(\"rel_cust_amount_to_max\", (sdf_mod['tx_amount']-sdf_mod['cust_amount_min'])\\\n",
    "                       / (sdf_mod['cust_amount_max'] - sdf_mod['cust_amount_min'] + eps)) \n",
    "           .withColumn(\"rel_term_amount_to_max\", (sdf_mod['tx_amount']-sdf_mod['term_amount_min'])\\\n",
    "                       / (sdf_mod['term_amount_max'] - sdf_mod['term_amount_min'] + eps))\n",
    "           .withColumnRenamed('tx_fraud', 'target') \n",
    "            \n",
    "           )\n",
    "\n",
    "\n",
    "train_sdf.printSchema()\n",
    "\n",
    "# sdf_mod.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3985feef",
   "metadata": {},
   "outputs": [],
   "source": [
    "numericColumns = list(\n",
    "    map(\n",
    "        lambda x: x[0], \n",
    "        filter(lambda x: x[1] == \"double\" or x[1] == \"int\", sdf.dtypes)\n",
    "    )\n",
    ")\n",
    "numericColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e755a348",
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = sdf.withColumn(\n",
    "    \"target\", \n",
    "    F.when(sdf[\"tx_fraud\"] == 1, 1).otherwise(0)\n",
    ")\n",
    "\n",
    "dft.select(\"tx_fraud\", \"target\").limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9398eda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dft.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8658a415",
   "metadata": {},
   "outputs": [],
   "source": [
    "dft.select(\"tx_fraud\", \"target\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f8e019",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "numericColumnsPairs = list(combinations(numericColumns, 2))\n",
    "numericColumnsPairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee51b186",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "corr = list(\n",
    "    filter(\n",
    "        lambda x: abs(x[2]) > 0.6, \n",
    "        map(lambda x: (x[0], x[1], dft.stat.corr(x[0], x[1])), numericColumnsPairs)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfb44bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr.sort(key=lambda x: x[2])\n",
    "for i in reversed(corr):\n",
    "    print(f\"{i[0]:>25} {i[1]:>25}\\t{i[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5257b127",
   "metadata": {},
   "outputs": [],
   "source": [
    "numericColumnsFinal = list(\n",
    "    set(numericColumns) - set(map(lambda x: x[1], corr))\n",
    ")\n",
    "\n",
    "numericColumnsFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e84f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "numericColumnsFinal =['term_amount_min',\n",
    " 'term_amount_50perc',\n",
    " 'term_amount_max',\n",
    " 'tx_amount',\n",
    " 'term_avg_amount_in_day_7d',\n",
    " 'sh_bad_trans_per_cust',\n",
    " 'cust_cnt_in_day_7d',\n",
    "# 'tx_fraud',\n",
    " 'rel_cust_50perc',\n",
    " 'sh_bad_days_per_term',\n",
    " 'rel_cust_amount_to_max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2477377",
   "metadata": {},
   "outputs": [],
   "source": [
    "numericColumnsFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c757a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# количество ноликов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca1a1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_zeros=sdf.filter(F.col('tx_fraud')==0).count()\n",
    "cnt_ones=sdf.filter(F.col('tx_fraud')==1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4e1b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10361194",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cc263f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "featureColumns = numericColumnsFinal# + catColumns\n",
    "\n",
    "assembler = VectorAssembler()\\\n",
    "    .setInputCols(featureColumns)\\\n",
    "    .setOutputCol(\"features\")\n",
    "\n",
    "assembled = assembler.transform(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecf6a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembled.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c210d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\\\n",
    "    .setInputCol(\"features\")\\\n",
    "    .setOutputCol(\"scaledFeatures\")\n",
    "\n",
    "scaled = scaler.fit(assembled).transform(assembled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cab0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\\\n",
    "    .setMaxIter(1000)\\\n",
    "    .setRegParam(0.2)\\\n",
    "    .setElasticNetParam(0.8)\\\n",
    "    .setFamily(\"binomial\")\\\n",
    "    .setFeaturesCol(\"scaledFeatures\")\\\n",
    "    .setLabelCol(\"target\")\n",
    "\n",
    "lrModel = lr.fit(scaled)\n",
    "\n",
    "print(f\"Coefficients: {lrModel.coefficients}\\nIntercept: {lrModel.intercept}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b0cafc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682b5682",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d74fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    labelCol = 'target',\n",
    "    featuresCol = 'features',\n",
    "    numTrees = 100\n",
    "    )\n",
    "\n",
    "\n",
    "# lr = LogisticRegression()\\\n",
    "#     .setMaxIter(1000)\\\n",
    "#     .setRegParam(0.2)\\\n",
    "#     .setElasticNetParam(0.8)\\\n",
    "#     .setFamily(\"binomial\")\\\n",
    "#     .setFeaturesCol(\"features\")\\\n",
    "#     .setLabelCol(\"target\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5e7f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model =rf.fit(assembled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfabfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8035d5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler()\\\n",
    "    .setInputCols(featureColumns)\\\n",
    "    .setOutputCol(\"features\")\n",
    "\n",
    "scaler = MinMaxScaler()\\\n",
    "    .setInputCol(\"features\")\\\n",
    "    .setOutputCol(\"scaledFeatures\")\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    labelCol = 'target',\n",
    "    featuresCol = 'scaledFeatures',\n",
    "    numTrees = 100\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48344db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8512a804",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = [assembler,scaler,rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8929ffc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(train_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ffbd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.functions import vector_to_array\n",
    "th = 0.2\n",
    "predictions = model.transform(test_sdf)\n",
    "\n",
    "predictions = (predictions\n",
    "              .withColumn('probability_arr', vector_to_array('probability'))\n",
    "              .withColumn('probability_one', F.col('probability_arr')[1])\n",
    "              .withColumn('pred_loc', \n",
    "                    F.when(F.col('probability_one') >= th, F.lit(1))\n",
    "                    .otherwise(F.lit(0))) \n",
    "              )\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8b0146",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf2f578",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = predictions.filter((F.col(\"target\") == 1) & (F.col(\"pred_loc\") == 1)).count()\n",
    "tn = predictions.filter((F.col(\"target\") == 0) & (F.col(\"pred_loc\") == 0)).count()\n",
    "fp = predictions.filter((F.col(\"target\") == 0) & (F.col(\"pred_loc\") == 1)).count()\n",
    "fn = predictions.filter((F.col(\"target\") == 1) & (F.col(\"pred_loc\") == 0)).count()\n",
    "\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1 = 2*tp / (2*tp + fp +fn)\n",
    "beta = 1.5\n",
    "f_bet = (1+beta*beta)*tp / ((1+beta*beta)*tp+fp+beta*beta*fn)\n",
    "\n",
    "print(f\"Trashhold={th}\")\n",
    "print(f\"Accuracy = {accuracy}\")\n",
    "print(f\"Precision = {precision}\")\n",
    "print(f\"Recall = {recall}\")\n",
    "print(f\"f1 = {f1}\")\n",
    "print(f\"f2 = {f_bet}\")\n",
    "print(f\"Confusion Matrix:\\n{tp:>4}\\t{fp:>4}\\n{fn:>4}\\t{tn:>4}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6c7dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a1ae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator()\\\n",
    "            .setLabelCol('target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c606f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = evaluator.evaluate(model.transform(test_sdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec373e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355a16f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049ec4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fa125b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transform(test_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec5070b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce30aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select('scaledFeatures','prediction','target','probability').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18de2a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.types import IntegerType,LongType,DoubleType,StringType,ArrayType\n",
    "\n",
    "# Функция для опреледения cause\n",
    "#@F.udf(returnType = T.StringType())\n",
    "@F.udf(returnType = DoubleType())\n",
    "def get_prob_ones(x):\n",
    "    print(x)\n",
    "    #return x[0][0][1]\n",
    "    return x#[0][0][0]\n",
    "    \n",
    "#     if x in ('1', '2', '17', '18'):\n",
    "#         return 'block'\n",
    "#     elif x in ('8', '9', '21', '34', '42', '50', '55'):\n",
    "#         return 'reruting'\n",
    "#     elif x == 'unknown':\n",
    "#         return 'unknown'\n",
    "#     return 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9da4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a= predictions.select('probability').take(1)#.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb24cc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b3cbdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82707db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "1535752*0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b89ab50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b7a74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(a[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11688357",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dd0a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e12df6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_col=F.split(F.col('probability'),\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a3d043",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType,LongType,DoubleType,StringType,ArrayType,StructType,StructField\n",
    "\n",
    "sheme = StructType([\n",
    "    StructField('prob_zero',DoubleType()),\n",
    "    StructField('prob_ones',DoubleType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c9fe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce27181f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorSlicer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2cabab",
   "metadata": {},
   "outputs": [],
   "source": [
    "slicer = VectorSlicer(inputCol = 'probability',outputCol = 'prob_one',indices =[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d38095d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = slicer.transform(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d10cd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2.select('probability','prob_one').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eb8819",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.functions import vector_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c056221",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions3 = (predictions2\n",
    "                 .withColumn('prob_one2', vector_to_array('prob_one'))\n",
    "                 .withColumn('prob_one3', F.col('prob_one2')[0])\n",
    "#                 .withColumn('probe_one_class', F.transform(F.col('probability'), lambda x: x[0]))\n",
    "                #.withColumn('probe_one_class', split_col.getItem(0))\n",
    "                #.withColumn('probe_one_class', get_prob_ones(F.col('probability')))\n",
    "                \n",
    "#                .withColumn('prob_ones2', F.struct(F.col('probability'),'array'))\n",
    "               \n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632d6d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions3.select('prob_one2','prob_one','prob_one3').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526e47db",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f966fb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = (predictions\n",
    "                .sample(0.0001)\n",
    "#                 .withColumn('probe_one_class', F.transform(F.col('probability'), lambda x: x[0]))\n",
    "                #.withColumn('probe_one_class', split_col.getItem(0))\n",
    "                #.withColumn('probe_one_class', get_prob_ones(F.col('probability')))\n",
    "               #.withColumn('prob_ones', F.struct(F.col('probability'),'array'))\n",
    "               \n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c03816d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2.select('prediction','target','probability','prob_ones').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9c7e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554a63bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = predictions.filter((F.col(\"target\") == 1) & (F.col(\"prediction\") == 1)).count()\n",
    "tn = predictions.filter((F.col(\"target\") == 0) & (F.col(\"prediction\") == 0)).count()\n",
    "fp = predictions.filter((F.col(\"target\") == 0) & (F.col(\"prediction\") == 1)).count()\n",
    "fn = predictions.filter((F.col(\"target\") == 1) & (F.col(\"prediction\") == 0)).count()\n",
    "\n",
    "print(f\"Confusion Matrix:\\n{tp:>4}\\t{fp:>4}\\n{fn:>4}\\t{tn:>4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d626ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "\n",
    "print(f\"Accuracy = {accuracy}\")\n",
    "print(f\"Precision = {precision}\")\n",
    "print(f\"Recall = {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04190b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc64338d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions4 = predictions3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9669731",
   "metadata": {},
   "outputs": [],
   "source": [
    "treshholds = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8]\n",
    "\n",
    "for th in treshholds:\n",
    "    predictions4 = (\n",
    "        predictions3\n",
    "        .withColumn('pred_loc', \n",
    "                F.when(F.col('prob_one3') >= th, F.lit(1))\n",
    "                .otherwise(F.lit(0)))\n",
    "    )\n",
    "    \n",
    "    tp = predictions4.filter((F.col(\"target\") == 1) & (F.col(\"pred_loc\") == 1)).count()\n",
    "    tn = predictions4.filter((F.col(\"target\") == 0) & (F.col(\"pred_loc\") == 0)).count()\n",
    "    fp = predictions4.filter((F.col(\"target\") == 0) & (F.col(\"pred_loc\") == 1)).count()\n",
    "    fn = predictions4.filter((F.col(\"target\") == 1) & (F.col(\"pred_loc\") == 0)).count()\n",
    "    \n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2*tp / (2*tp + fp +fn)\n",
    "    beta = 1.5\n",
    "    f_bet = (1+beta*beta)*tp / ((1+beta*beta)*tp+fp+beta*beta*fn)\n",
    "    \n",
    "    print(f\"Trashhold={th}\")\n",
    "    print(f\"Accuracy = {accuracy}\")\n",
    "    print(f\"Precision = {precision}\")\n",
    "    print(f\"Recall = {recall}\")\n",
    "    print(f\"f1 = {f1}\")\n",
    "    print(f\"f2 = {f_bet}\")\n",
    "    print(f\"Confusion Matrix:\\n{tp:>4}\\t{fp:>4}\\n{fn:>4}\\t{tn:>4}\")\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c126225",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4bf9bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9defeb6c",
   "metadata": {},
   "source": [
    "## clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b36d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sdf = datamart(time_keys[23])\n",
    "test_sdf = datamart(time_keys[24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59ac879",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bea123",
   "metadata": {},
   "outputs": [],
   "source": [
    "numericColumnsFinal =['term_amount_min',\n",
    " 'term_amount_50perc',\n",
    " 'term_amount_max',\n",
    " 'tx_amount',\n",
    " 'term_avg_amount_in_day_7d',\n",
    " 'sh_bad_trans_per_cust',\n",
    " 'cust_cnt_in_day_7d',\n",
    "# 'tx_fraud',\n",
    " 'rel_cust_50perc',\n",
    " 'sh_bad_days_per_term',\n",
    " 'rel_cust_amount_to_max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f152a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = train_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5d869f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "featureColumns = numericColumnsFinal# + catColumns\n",
    "\n",
    "assembler = VectorAssembler()\\\n",
    "    .setInputCols(featureColumns)\\\n",
    "    .setOutputCol(\"features\")\n",
    "\n",
    "assembled = assembler.transform(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b87df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\\\n",
    "    .setInputCol(\"features\")\\\n",
    "    .setOutputCol(\"scaledFeatures\")\n",
    "\n",
    "scaled = scaler.fit(assembled).transform(assembled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e046a1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20506cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    labelCol = 'target',\n",
    "    featuresCol = 'features',\n",
    "    numTrees = 100\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691295a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model =rf.fit(assembled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84551780",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043e8e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler()\\\n",
    "    .setInputCols(featureColumns)\\\n",
    "    .setOutputCol(\"features\")\n",
    "\n",
    "scaler = MinMaxScaler()\\\n",
    "    .setInputCol(\"features\")\\\n",
    "    .setOutputCol(\"scaledFeatures\")\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    labelCol = 'target',\n",
    "    featuresCol = 'scaledFeatures',\n",
    "    numTrees = 100\n",
    "    )\n",
    "\n",
    "pipeline = Pipeline(stages = [assembler,scaler,rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad67302",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(train_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3497caaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\\\n",
    "            .setLabelCol('target')\n",
    "\n",
    "auc = evaluator.evaluate(model.transform(test_sdf))\n",
    "\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e859cb3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce48f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.functions import vector_to_array\n",
    "th = 0.2\n",
    "predictions = model.transform(test_sdf)\n",
    "\n",
    "predictions = (predictions\n",
    "              .withColumn('probability_arr', vector_to_array('probability'))\n",
    "              .withColumn('probability_one', F.col('probability_arr')[1])\n",
    "              .withColumn('pred_loc', \n",
    "                    F.when(F.col('probability_one') >= th, F.lit(1))\n",
    "                    .otherwise(F.lit(0))) \n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67f2d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = predictions.filter((F.col(\"target\") == 1) & (F.col(\"pred_loc\") == 1)).count()\n",
    "tn = predictions.filter((F.col(\"target\") == 0) & (F.col(\"pred_loc\") == 0)).count()\n",
    "fp = predictions.filter((F.col(\"target\") == 0) & (F.col(\"pred_loc\") == 1)).count()\n",
    "fn = predictions.filter((F.col(\"target\") == 1) & (F.col(\"pred_loc\") == 0)).count()\n",
    "\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1 = 2*tp / (2*tp + fp +fn)\n",
    "beta = 1.5\n",
    "f_bet = (1+beta*beta)*tp / ((1+beta*beta)*tp+fp+beta*beta*fn)\n",
    "\n",
    "print(f\"Trashhold={th}\")\n",
    "print(f\"Accuracy = {accuracy}\")\n",
    "print(f\"Precision = {precision}\")\n",
    "print(f\"Recall = {recall}\")\n",
    "print(f\"f1 = {f1}\")\n",
    "print(f\"f2 = {f_bet}\")\n",
    "print(f\"Confusion Matrix:\\n{tp:>4}\\t{fp:>4}\\n{fn:>4}\\t{tn:>4}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d388c893",
   "metadata": {},
   "source": [
    "### hyper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517782f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36fc91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c561d981",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "#import mlflow\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, SparkTrials, Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f725c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определяем пространство поиска для hyperopt\n",
    "search_space = {\n",
    "    'numTrees': hp.randint('numTrees', 50, 150),\n",
    "    'maxDepth': hp.randint('maxDepth', 3,7)\n",
    "}\n",
    "\n",
    "def objective(params, train_data, test_data):\n",
    "    print(params)\n",
    "    \n",
    "    assembler = VectorAssembler()\\\n",
    "    .setInputCols(featureColumns)\\\n",
    "    .setOutputCol(\"features\")\n",
    "\n",
    "    scaler = MinMaxScaler()\\\n",
    "        .setInputCol(\"features\")\\\n",
    "        .setOutputCol(\"scaledFeatures\")\n",
    "    \n",
    "    rf = RandomForestClassifier()\\\n",
    "        .setFeaturesCol('scaledFeatures')\\\n",
    "        .setLabelCol('target')\\\n",
    "        .setMaxDepth(params['maxDepth'])\\\n",
    "        .setNumTrees(params['numTrees'])\\\n",
    "\n",
    "    pipeline = Pipeline(stages = [assembler,scaler,rf])\n",
    "\n",
    "#     lr = LogisticRegression()\\\n",
    "#         .setMaxIter(1000)\\\n",
    "#         .setRegParam(params['regParam'])\\\n",
    "#         .setFeaturesCol('Features')\\\n",
    "#         .setLabelCol('Survived')\n",
    "\n",
    "#     evaluator = BinaryClassificationEvaluator()\\\n",
    "#             .setLabelCol('Survived')\n",
    "\n",
    "    rf_model = pipeline.fit(train_data)\n",
    "\n",
    "    auc = evaluator.evaluate(rf_model.transform(test_data))\n",
    "\n",
    "#     with mlflow.start_run():\n",
    "#         mlflow.log_params(params)\n",
    "#         mlflow.log_metric('auc', auc)\n",
    "    \n",
    "    return {'loss': -auc, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc591e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "\n",
    "# mlflow.set_experiment('classification')\n",
    "\n",
    "best = fmin(\n",
    "    fn=partial(\n",
    "        objective, \n",
    "        train_data=train_sdf,\n",
    "        test_data=test_sdf\n",
    "    ),\n",
    "    space=search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=10,\n",
    "    trials=trials\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c33ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1c6805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f895bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f15acee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3ecdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = train_sdf.withColumn(\n",
    "    \"target\", \n",
    "    F.when(train_sdf[\"tx_fraud\"] == 1, 1).otherwise(0)\n",
    ")\n",
    "\n",
    "dft.select(\"tx_fraud\", \"target\").limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e72898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0ef23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install loguru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43728a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)-15s %(message)s\")\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc97ac15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a814556",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Creating Spark Session ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765b75cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b366e0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datamart(date_loc):\n",
    "    \n",
    "    eps = 1e-6\n",
    "\n",
    "    df = (row_sdf\n",
    "               .filter(F.col('date_key') == date_loc)\n",
    "               .join(agg_term_sdf, on=['terminal_id','date_key'], how='left')\n",
    "               .join(agg_cust_sdf, on=['customer_id','date_key'], how='left')\n",
    "               .fillna(value=0,subset=list_for_fillna)\n",
    "               #.withColumn(\"sh_bad_trans_per_cust\", sdf['cust_bad_cnt_trans_7d'] / (sdf.cust_total_cnt_trans_7d + eps))\n",
    "    #            .withColumn(\"sh_bad_trans_per_term\", sdf.term_bad_cnt_trans_7d / (sdf.term_total_cnt_trans_7d + eps))\n",
    "              )\n",
    "\n",
    "    #sdf.printSchema()\n",
    "\n",
    "    df = (df\n",
    "\n",
    "\n",
    "               .withColumn(\"sh_bad_trans_per_cust\", df['cust_bad_cnt_trans_7d'] / (df['cust_total_cnt_trans_7d'] + eps))\n",
    "               .withColumn(\"sh_bad_trans_per_term\", df['term_bad_cnt_trans_7d'] / (df['term_total_cnt_trans_7d'] + eps))\n",
    "               .withColumn(\"sh_bad_days_per_cust\", df['cust_days_with_bad_trans_7d'] / (df['cust_active_days_7d'] + eps))\n",
    "               .withColumn(\"sh_bad_days_per_term\", df['term_days_with_bad_trans_7d'] / (df['term_active_days_7d'] + eps))\n",
    "               .withColumn(\"sh_bad_trans_per_cust\", df['cust_bad_cnt_trans_7d'] / (df['cust_total_cnt_trans_7d'] + eps))\n",
    "               .withColumn(\"rel_cust_amount_to_max\", (df['tx_amount']-df['cust_amount_min'])\\\n",
    "                           / (df['cust_amount_max'] - df['cust_amount_min'] + eps)) \n",
    "               .withColumn(\"rel_term_amount_to_max\", (df['tx_amount']-df['term_amount_min'])\\\n",
    "                           / (df['term_amount_max'] - df['term_amount_min'] + eps))\n",
    "               .withColumnRenamed('tx_fraud', 'target') \n",
    "\n",
    "               )\n",
    "\n",
    "\n",
    "    #train_sdf.printSchema()\n",
    "    return df\n",
    "\n",
    "search_space = {\n",
    "    'numTrees': hp.randint('numTrees', 50, 150),\n",
    "    'maxDepth': hp.randint('maxDepth', 3,7)\n",
    "}\n",
    "\n",
    "def objective(params, train_data, test_data):\n",
    "    print(params)\n",
    "    \n",
    "    assembler = VectorAssembler()\\\n",
    "    .setInputCols(featureColumns)\\\n",
    "    .setOutputCol(\"features\")\n",
    "\n",
    "    scaler = MinMaxScaler()\\\n",
    "        .setInputCol(\"features\")\\\n",
    "        .setOutputCol(\"scaledFeatures\")\n",
    "    \n",
    "    rf = RandomForestClassifier()\\\n",
    "        .setFeaturesCol('scaledFeatures')\\\n",
    "        .setLabelCol('target')\\\n",
    "        .setMaxDepth(params['maxDepth'])\\\n",
    "        .setNumTrees(params['numTrees'])\\\n",
    "\n",
    "    pipeline = Pipeline(stages = [assembler,scaler,rf])\n",
    "\n",
    "    rf_model = pipeline.fit(train_data)\n",
    "\n",
    "    auc = evaluator.evaluate(rf_model.transform(test_data))\n",
    "\n",
    "#     with mlflow.start_run():\n",
    "#         mlflow.log_params(params)\n",
    "#         mlflow.log_metric('auc', auc)\n",
    "    \n",
    "    return {'loss': -auc, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114976e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'cold-s3-bucket'\n",
    "row_path = f\"s3a://{bucket_name}/output_data/clean_data.parquet\"\n",
    "\n",
    "row_sdf = spark.read.parquet(row_path)\n",
    "\n",
    "#row_sdf.printSchema()\n",
    "\n",
    "agg_cust_path = f\"s3a://{bucket_name}/output_data/agg_customer_data.parquet\"\n",
    "agg_cust_sdf = spark.read.parquet(agg_cust_path)\n",
    "#agg_cust_sdf.printSchema()\n",
    "\n",
    "agg_term_path = f\"s3a://{bucket_name}/output_data/agg_term_data.parquet\"\n",
    "agg_term_sdf = spark.read.parquet(agg_term_path)\n",
    "#agg_term_sdf.printSchema()  \n",
    "\n",
    "\n",
    "list_for_fillna = [\n",
    "'term_active_days_7d',\n",
    " 'term_uniq_customer_7d',\n",
    " 'term_amount_50perc',\n",
    " 'term_total_cnt_trans_7d',\n",
    " 'term_amount_max',\n",
    " 'term_amount_min',\n",
    " 'term_bad_cnt_trans_7d',\n",
    " 'term_avg_amount_in_day_7d',\n",
    " 'term_cnt_in_day_7d',\n",
    " 'term_days_with_bad_trans_7d',\n",
    " 'cust_active_days_7d',\n",
    " 'cust_uniq_terminal_7d',\n",
    " 'cust_amount_50perc',\n",
    " 'cust_amount_max',\n",
    " 'cust_amount_min',\n",
    " 'cust_total_cnt_trans_7d',\n",
    " 'cust_bad_cnt_trans_7d',\n",
    " 'cust_avg_amount_in_day_7d',\n",
    " 'cust_cnt_in_day_7d',\n",
    " 'cust_days_with_bad_trans_7d',\n",
    " 'rel_cust_50perc',\n",
    "#  'sh_bad_trans_per_cust',\n",
    "#  'sh_bad_trans_per_term',\n",
    "#  'sh_bad_days_per_cust',\n",
    "#  'sh_bad_days_per_term',\n",
    "#  'rel_cust_amount_to_max',\n",
    "#  'rel_term_amount_to_max'\n",
    "\n",
    "]\n",
    "\n",
    "list_dates= row_sdf.select('date_key').distinct().collect()\n",
    "ld = [str(x[0]) for x in list_dates]\n",
    "start=min(ld)\n",
    "end=max(ld)\n",
    "time_keys = [\n",
    "    time_key.strftime(\"%Y-%m-%d\") for time_key\n",
    "    in pd.date_range(start, end, freq='1D')\n",
    "]\n",
    "\n",
    "train_sdf = datamart(time_keys[23])\n",
    "test_sdf = datamart(time_keys[24])\n",
    "\n",
    "numericColumnsFinal =['term_amount_min',\n",
    "     'term_amount_50perc',\n",
    "     'term_amount_max',\n",
    "     'tx_amount',\n",
    "     'term_avg_amount_in_day_7d',\n",
    "     'sh_bad_trans_per_cust',\n",
    "     'cust_cnt_in_day_7d',\n",
    "    # 'tx_fraud',\n",
    "     'rel_cust_50perc',\n",
    "     'sh_bad_days_per_term',\n",
    "     'rel_cust_amount_to_max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16fe7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "\n",
    "    # mlflow.set_experiment('classification')\n",
    "\n",
    "best = fmin(\n",
    "    fn=partial(\n",
    "        objective, \n",
    "        train_data=train_sdf,\n",
    "        test_data=test_sdf\n",
    "    ),\n",
    "    space=search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=2,\n",
    "    trials=trials\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f133fbb",
   "metadata": {},
   "source": [
    "### clean 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "471e5def",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe07757b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from loguru import logger\n",
    "from functools import partial\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame, functions as F\n",
    "from pyspark.sql.types import IntegerType,LongType,DoubleType,StringType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.types import IntegerType,LongType,DoubleType,StringType,ArrayType\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, SparkTrials, Trials\n",
    "#import mlflow\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2998b51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def datamart(date_loc):\n",
    "    \n",
    "    eps = 1e-6\n",
    "\n",
    "    df = (row_sdf\n",
    "               .filter(F.col('date_key') == date_loc)\n",
    "               .join(agg_term_sdf, on=['terminal_id','date_key'], how='left')\n",
    "               .join(agg_cust_sdf, on=['customer_id','date_key'], how='left')\n",
    "               .fillna(value=0,subset=list_for_fillna)\n",
    "               #.withColumn(\"sh_bad_trans_per_cust\", sdf['cust_bad_cnt_trans_7d'] / (sdf.cust_total_cnt_trans_7d + eps))\n",
    "    #            .withColumn(\"sh_bad_trans_per_term\", sdf.term_bad_cnt_trans_7d / (sdf.term_total_cnt_trans_7d + eps))\n",
    "              )\n",
    "\n",
    "    #sdf.printSchema()\n",
    "\n",
    "    df = (df\n",
    "\n",
    "\n",
    "               .withColumn(\"sh_bad_trans_per_cust\", df['cust_bad_cnt_trans_7d'] / (df['cust_total_cnt_trans_7d'] + eps))\n",
    "               .withColumn(\"sh_bad_trans_per_term\", df['term_bad_cnt_trans_7d'] / (df['term_total_cnt_trans_7d'] + eps))\n",
    "               .withColumn(\"sh_bad_days_per_cust\", df['cust_days_with_bad_trans_7d'] / (df['cust_active_days_7d'] + eps))\n",
    "               .withColumn(\"sh_bad_days_per_term\", df['term_days_with_bad_trans_7d'] / (df['term_active_days_7d'] + eps))\n",
    "               .withColumn(\"sh_bad_trans_per_cust\", df['cust_bad_cnt_trans_7d'] / (df['cust_total_cnt_trans_7d'] + eps))\n",
    "               .withColumn(\"rel_cust_amount_to_max\", (df['tx_amount']-df['cust_amount_min'])\\\n",
    "                           / (df['cust_amount_max'] - df['cust_amount_min'] + eps)) \n",
    "               .withColumn(\"rel_term_amount_to_max\", (df['tx_amount']-df['term_amount_min'])\\\n",
    "                           / (df['term_amount_max'] - df['term_amount_min'] + eps))\n",
    "               .withColumnRenamed('tx_fraud', 'target') \n",
    "\n",
    "               )\n",
    "\n",
    "\n",
    "    #train_sdf.printSchema()\n",
    "    return df\n",
    "\n",
    "\n",
    "# Определяем пространство поиска для hyperopt\n",
    "search_space = {\n",
    "    'numTrees': hp.randint('numTrees', 50, 150),\n",
    "    'maxDepth': hp.randint('maxDepth', 3,7)\n",
    "}\n",
    "\n",
    "\n",
    "def objective(params, train_data, test_data):\n",
    "    logger.info(f\"params {params}\")\n",
    "\n",
    "    \n",
    "    assembler = VectorAssembler()\\\n",
    "    .setInputCols(featureColumns)\\\n",
    "    .setOutputCol(\"features\")\n",
    "\n",
    "    scaler = MinMaxScaler()\\\n",
    "        .setInputCol(\"features\")\\\n",
    "        .setOutputCol(\"scaledFeatures\")\n",
    "    \n",
    "    rf = RandomForestClassifier()\\\n",
    "        .setFeaturesCol('scaledFeatures')\\\n",
    "        .setLabelCol('target')\\\n",
    "        .setMaxDepth(params['maxDepth'])\\\n",
    "        .setNumTrees(params['numTrees'])\\\n",
    "\n",
    "    pipeline = Pipeline(stages = [assembler,scaler,rf])\n",
    "\n",
    "    rf_model = pipeline.fit(train_data)\n",
    "    \n",
    "    evaluator = BinaryClassificationEvaluator()\\\n",
    "            .setLabelCol('target')\n",
    "\n",
    "    auc = evaluator.evaluate(rf_model.transform(test_data))\n",
    "\n",
    "#     with mlflow.start_run():\n",
    "#         mlflow.log_params(params)\n",
    "#         mlflow.log_metric('auc', auc)\n",
    "    \n",
    "    return {'loss': -auc, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6b82c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-10 16:28:08.739\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m1\u001b[0m - \u001b[1mCreating Spark Session ...\u001b[0m\n",
      "\u001b[32m2025-01-10 16:28:17.982\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1m<pyspark.sql.session.SparkSession object at 0x7fb2423a4790>\u001b[0m\n",
      "\u001b[32m2025-01-10 16:28:29.064\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mdata upload ...\u001b[0m\n",
      "\u001b[32m2025-01-10 16:28:36.613\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mtraind perion 2022-09-28 test period 2022-09-29\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-10 16:28:37.171\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mobjective\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mparams {'maxDepth': 5, 'numTrees': 91}\u001b[0m\n",
      "job exception: name 'evaluator' is not defined\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [02:04<?, ?trial/s, best loss=?]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'evaluator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e9cc9cf4960d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;31m# mlflow.set_experiment('classification')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m best = fmin(\n\u001b[0m\u001b[1;32m     93\u001b[0m     fn=partial(\n\u001b[1;32m     94\u001b[0m         \u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mallow_trials_fmin\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fmin\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m         return trials.fmin(\n\u001b[0m\u001b[1;32m    541\u001b[0m             \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfmin\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         return fmin(\n\u001b[0m\u001b[1;32m    672\u001b[0m             \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0;31m# next line is where the fmin is actually executed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                     \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"job exception: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    890\u001b[0m                 \u001b[0mprint_node_on_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrec_eval_print_node_on_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             )\n\u001b[0;32m--> 892\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-400a33ad4442>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(params, train_data, test_data)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mrf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mauc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;31m#     with mlflow.start_run():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluator' is not defined"
     ]
    }
   ],
   "source": [
    "logger.info(\"Creating Spark Session ...\")\n",
    "  \n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName('Spark ML Research')\\\n",
    "    .config('spark.sql.repl.eagerEval.enabled', True) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "logger.info(spark)\n",
    "\n",
    "bucket_name = 'cold-s3-bucket'\n",
    "row_path = f\"s3a://{bucket_name}/output_data/clean_data.parquet\"\n",
    "\n",
    "row_sdf = spark.read.parquet(row_path)\n",
    "\n",
    "#row_sdf.printSchema()\n",
    "\n",
    "agg_cust_path = f\"s3a://{bucket_name}/output_data/agg_customer_data.parquet\"\n",
    "agg_cust_sdf = spark.read.parquet(agg_cust_path)\n",
    "#agg_cust_sdf.printSchema()\n",
    "\n",
    "agg_term_path = f\"s3a://{bucket_name}/output_data/agg_term_data.parquet\"\n",
    "agg_term_sdf = spark.read.parquet(agg_term_path)\n",
    "#agg_term_sdf.printSchema()  \n",
    "\n",
    "logger.info(\"data upload ...\")\n",
    "\n",
    "list_for_fillna = [\n",
    "'term_active_days_7d',\n",
    " 'term_uniq_customer_7d',\n",
    " 'term_amount_50perc',\n",
    " 'term_total_cnt_trans_7d',\n",
    " 'term_amount_max',\n",
    " 'term_amount_min',\n",
    " 'term_bad_cnt_trans_7d',\n",
    " 'term_avg_amount_in_day_7d',\n",
    " 'term_cnt_in_day_7d',\n",
    " 'term_days_with_bad_trans_7d',\n",
    " 'cust_active_days_7d',\n",
    " 'cust_uniq_terminal_7d',\n",
    " 'cust_amount_50perc',\n",
    " 'cust_amount_max',\n",
    " 'cust_amount_min',\n",
    " 'cust_total_cnt_trans_7d',\n",
    " 'cust_bad_cnt_trans_7d',\n",
    " 'cust_avg_amount_in_day_7d',\n",
    " 'cust_cnt_in_day_7d',\n",
    " 'cust_days_with_bad_trans_7d',\n",
    " 'rel_cust_50perc',\n",
    "#  'sh_bad_trans_per_cust',\n",
    "#  'sh_bad_trans_per_term',\n",
    "#  'sh_bad_days_per_cust',\n",
    "#  'sh_bad_days_per_term',\n",
    "#  'rel_cust_amount_to_max',\n",
    "#  'rel_term_amount_to_max'\n",
    "\n",
    "]\n",
    "\n",
    "list_dates= row_sdf.select('date_key').distinct().collect()\n",
    "ld = [str(x[0]) for x in list_dates]\n",
    "start=min(ld)\n",
    "end=max(ld)\n",
    "time_keys = [\n",
    "    time_key.strftime(\"%Y-%m-%d\") for time_key\n",
    "    in pd.date_range(start, end, freq='1D')\n",
    "]\n",
    "\n",
    "logger.info(f\"traind perion {time_keys[23]} test period {time_keys[24]}\")\n",
    "\n",
    "train_sdf = datamart(time_keys[23])\n",
    "test_sdf = datamart(time_keys[24])\n",
    "\n",
    "numericColumnsFinal =['term_amount_min',\n",
    "     'term_amount_50perc',\n",
    "     'term_amount_max',\n",
    "     'tx_amount',\n",
    "     'term_avg_amount_in_day_7d',\n",
    "     'sh_bad_trans_per_cust',\n",
    "     'cust_cnt_in_day_7d',\n",
    "    # 'tx_fraud',\n",
    "     'rel_cust_50perc',\n",
    "     'sh_bad_days_per_term',\n",
    "     'rel_cust_amount_to_max']\n",
    "featureColumns = numericColumnsFinal\n",
    "\n",
    "#mlflow.set_experiment('classification')\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "# mlflow.set_experiment('classification')\n",
    "\n",
    "best = fmin(\n",
    "    fn=partial(\n",
    "        objective, \n",
    "        train_data=train_sdf,\n",
    "        test_data=test_sdf\n",
    "    ),\n",
    "    space=search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=2,\n",
    "    trials=trials\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "654a6ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
